This text file serves as some interim documentation while the code
evolves. Feel free to edit this file, add corrections and questions,
etc. New updates will go into the top of this file in the future.

16 MAY 2018 EXECUTIVE SUMMARY

Haplotree creation is working at a basic level. This means a set of
datafiles can be run through the algorithm, producing a graphable tree
showing clades that were automatically detected. The data is also
available as a spreadsheet of ones and zeroes (one representing a
positive call for a SNP for a person). This is easily demonstrated using
the existing code and pretty much any set of kits from the repository,
even kits that do not belong to P312 or U106.

The storage system is fairly complete and separate from the analysis,
with APIs for extracting call data from the database.

Kit loading works for all FTDNA b38 data from the Haplogroup-R
repository. This means metadata for the kits is automatically populated
from the Haplogroup-R web API, and any .zip files that have been
downloaded into a local folder can be parsed and loaded into the
database. This makes the kit data available for analysis. Kit data can
be loaded into the database incrementally. The parsing and loading of
kit data works on the order of 10 per minute on my machine, and this is
a one-time hit.

There are no strange dependencies, and likely anyone could pick up the
code from the git repo and run it, assuming they have access to some
data and python3. It may be simpler on Linux/UNIX as that's the dev
environment, but there's no fundamental reason it wouldn't work
somewhere else.

You said good news first, right? That was the good news. So there are
some significant limitations. Exceeding about 150 kits at a time in the
haplotree generation seriously runs into scaling issues. The
inconsistent calls are set aside and not used. Missing calls can cause
mistakes in the tree. There is not yet a good rules processing system in
place to correct such errors and omissions.

Set up the source code directory. Clone from the git repository:
git clone https://github.com/jazdrv/dnaTools.git

Put your data files into the data directory, under the HaplogroupR
directory. Use the same directory structure as on the
repository. Namely, the zip files should be under HaplogroupR/FTDNA/b38
and you should not rename the zip files.

How to load kit data:

Go to the src subdirectory

Edit config.yaml. The useful settings are described and near the top of
the file. For the initial setup, use at least these settings:
kitlimit: 10
use_web_api: True
drop_tables: True

You probably won't need to change other settings unless you want to.

Set up your environment variable for the script to know where the
directory is on your file system:
. ./redux.env

Load some kits
./redux.py -t

If this worked without errors, edit config.yaml again.
kitlimit: 100 (for example)
use_web_api: False
drop_tables: False

Load some more kits:
./redux.py -l (repeat as many times as you want - until all kits are loaded)


How to build a haplotree

If you want to use a particular set of kits, put the kit IDs in the file
"kits.txt." One kitID per line

If some of those kits aren't loaded yet, run the script again as above
to load kits. Loading prioritizes kits in kits.txt.

Build a sorted array from the call data for those kits.
./redux.py -o

Note: this is a little time-consuming, so the sorted array is cached in
../data/cache. Subsequent runs will load from the cache instead of
recomputing, as long as the kit list hasn't changed.

Caveat: cached data could be incorrect if something changed, such as the
exclude file (see below). If in doubt, remove the cached data and re-run
the sorted array generation. "rm ../data/cache/saved-data*"

Build the haplotree.
./redux.py -m > log.out 2>&1

View the haplotree.
xdot tree.gv
gnumeric out.csv
(you can also copy and paste tree.gv to webgraphviz.com)

There's some important output in the log.out file worth checking. In
detecting clades, the program reports inconsistencies as "not all
zeroes. This means in the 2d array (person x SNP) which has been sorted
into clade order, a block that was expected to be all zero (negative for
those people/calls) had some ones. These blocks are reported as tuples
that correspond to mincol,maxcol, minrow,maxrow, count in the array,
which you can look at in out.csv. These represent such things as
inconsistent calls, missing calls, recurrencies. For doing a proof of
concept, you don't need to resolve all of these, especially if they look
like one-off recurrencies. Most likely a more-or-less correct tree will
still be generated.

When you notice some problematic kits and/or problematic variants, you
can add those to excludes.csv, and they will be omitted from the
analysis. Changing excludes.csv most likely invalidates the cached
matrix, and you should remove the saved* files from ../data/cache


You can merge trees from two or more different runs. This works to some
extent, but it is not tolerant of significant errors and omissions in
the tree. It's still experimental. To try, just create a different
kits.txt for a new set of kits, re-run the array and haplotree as above,
and take a look at tree.gv. There is currently no combined .csv
generated. This could be done in the future as a different method of
rendering the tree. Errors accumulate in the tree. If you keep running
it over and over with different kits, you will end up with a spaghetti
mess of a tree.

Re-running the analysis.  There is no polished way to delete the tree
from a prior run. To start with a clean slate, run ./tree.py first. This
will delete all trees stored in the database (have a look at the code at
the bottom of tree.py. This is the current kludge for starting with a
clean slate.) Most of tree.py is still in early experimental stages.
