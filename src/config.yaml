---

# configuration variables for redux

# Source code version
VERSION: 2.0.0.20180108.ALPHA

# Where the source code lives. Some shell scripts depend on this as an
# environment variable, so this configuration item may need some more
# thought. For now, there's a .env file you can source as
# ". ./redux.env" to set the environment variable
# You may need to change this to a full pathname where your working directory is.
REDUX_ENV: .
REDUX_PATH: .

# The path to the directory where SQL files live (e.g. the schema file)
REDUX_SQL: ../sql

# The path to the directory for the data files (e.g. the .zip files)
REDUX_DATA: ../data

# The path to the directory for the related external programs
REDUX_BIN: ../bin

# HOW MUCH DO YOU WANT THE PROGRAM TO SHOUT AT YOU?
# the higher the number, the more output
verbosity: 3

# limit the number of kits that will be processed
# normally, process all kits available, so leave this at a high number
kitlimit: 5

# pull the list of kits using the HaplogroupR web API. THIS NEEDS TO BE
# DONE ONCE, but thereafter it can be false to use the locally cached
# copy of the list, depending on how often you want a new list of kits
use_web_api: False

# location of the data files, under REDUX_DATA
# zip_dir contains original zip files; unzip <may> contain extracted files
zip_dir: zip
unzip_dir: unzip

# the name of the file on disk for sqlite3
DB_FILE: variant.db

# the name of the hg19 and hg38 named SNP definitions files
# these should not need to be changed; they are pulled from the web
b37_snp_file: "snps_hg19.csv"
b38_snp_file: "snps_hg38.csv"

# currently used during the database creation phase to cause the
# database to be recreated every time. False means leave whatever exists
# in the database, but that is not necessarily working yet
drop_tables: True

# maximum age in days of SNP definitions files before we download a new
# copy from the web
max_snpdef_age: 3

#---------------------old or not yet used------------------

zero_age: 1950.33
err_zero_age: 15.02

DEBUG: 0

# special case extra args for the unpack-zip-files command
unpack_args: -s 6:1,2,3,4,5,6 
unpack_args:

# Notes on SKIPZIP
# Set SKIPZIP to >0 skip the labour-intensive parts of the
# script. Requires MAKEREPORT=TRUE This should only be done if you have
# run the script before on the same dataset!
# If set to 1, unzipping and VCF/BED file scanning will be ignored
#    Statistics generation, sorting of already-identified variants into
#    good/bad/shared lists and clade sorting will still take place This
#    requires a successful (but not necessarily sorted) previous run.
# If set to 2, statistics generation is skipped. The statistics
#    INCLUDING KIT NAMES will be taken from the header of the previous
#    run.  It also uses the ORDER of the previous run, taken from
#    order.txt This requires a complete successful previous run
# If set to 3, SNP names aren't updated either. The previous SNP names are used.
skip_zip: False

# ZIPUPDATEONLY = don't re-extract that already exist # There's a bug
# here somewhere!
zip_update_only: True


# CHECKDATA = display warnings and create lists if inconsistencies
# detected in report (0 = no; >0 = yes)
check_data: True

# Notes on TESTFORREFPOS
# This should be set for clades where the reference sequence contains
# positives For BigY, this includes clades in R1b-U152 and G2.  Clades
# which include ancestral SNPs of these regions (e.g. R, R1, R1b-M343,
# R1b-M269, R1b-L11, R1b-P312, ...)  should have this set. However,
# self-contained branches of these clades do not (e.g. R1b-U106) as test
# results should be all positive or all negative for the group.
test_for_ref_pos: False


# top_snp: defines what your top-level SNP is called out of the (usually
# large number of) possible shared SNP's


# These flags define what actually gets done

# Unless MAKEREPORT is non-zero, the other reports will be based on the current report.csv
# MAKEREPORT = do the main reduction and create report.csv, else use an existing one
# MAKEAGES = perform age analysis
# MAKESHORTREPORT = make a shorter version of the report, collapsing the singleton section and removing the shared and inconsistent list of SNPs
# MAKEHTMLTREE = make an HTML version of the tree for easier visualisation
# SKIPZIP = skip certain parts of the script: see below
# TESTFORREFPOS = test for positives in the references sequence and swap if needed: see below



# The following parameters set the mutation rate for the age analysis.
# The rate is set in SNP mutations per year per *billion* base pairs.
# RATE0 and RATE1 give the lower and upper bounds for the 95% confidence interval.
# The rate you should use varies depending on the region of the chromsome involved.
# Rates are numerically lower in palindromic regions (see Helgason et al. 2015 and our own analyses).

# Rates for entire BigY test
# RATE=0.751
# RATE0=0.688
# RATE1=0.814

# Rates for standardised age.bed restricted region
# RATE=0.8186
# RATE0=0.7589
# RATE1=0.8771
# Rates for new age.bed (v. 0.7.0)

# rate: 0.8119
# rate_0: 0.7529
# rate_1: 0.8716

# ZEROAGE gives the date from which ages are computed
# ERRZEROAGE gives the (95% c.i.) +/- variation in this date for individual testers

# That's it!

# List of required internal libraries:
#      positives-and-no-calls.py : This code reads the call status of SNPs from the BED files (Courtesy H.A.)
#      snps_hg19.csv : Contains SNP names. Can be updated, e.g.: wget http://ybrowse.org/gbrowse2/gff/snps_hg19.csv -O snps_hg19.csv
#      age.bed : Contains the regions used to count SNPs for age analysis.
#      poisson.tbl : A look-up table containing a matrix of Poisson functions
#      cpoisson.tbl : A look-up table containing 95% confidence intervals for cumulative Poisson statistics
#      events.svg : A customisable list of historical events to be included in the SVG file

# How to make a backup:
#      zip redux.zip redux.bash positives-and-no-calls.py age.bed poisson.tbl cpoisson.tbl implications.txt badlist.txt recurrencies.txt cladenames.txt events.svg treefoot.html merge-ignore.txt snps_hg19.csv 

# How to update cladenames.txt from a list of clades and substitutes in <official-tree.csv> formatted as:
# PARENT_SNP,
# ,CHILD_SNP
# ,CHILD_SNP
# awk 'NR==FNR {n[NR]=$1;o[NR]=$2;x=NR} NR!=FNR {for (i=1;i<=x;i++) if ($1==o[i] && $1!=n[i] && $1+0==0) print o[i],n[i]}' <(awk '$1!="" {split($1,pp," "); split(pp[1],p,"/")} $2!="" {split($2,qq," "); split(qq[1],q,"/")} NR>3 {if ($1=="") print p[1],q[1]; if ($2=="") print p[1],p[1]}' FS=, official-tree.csv  | sed 's/In://Ig' | grep '[0-9]') <(awk '{print $16}' final-ages.txt) >> cladenames.txt


# Set start date for timing points
# Note: I believe this needs to be taken out of YAML. keeping her for now.
# T0: `date +%s.%N`
# This is the number of columns on the left-hand side of the CSV reports
# left_cols: 17
# Use is currently under development



# The list of SNPs used as a reference
# Get this using:
# call(["source","webupdate.scr"])
# which runs:
#   wget http://ybrowse.org/gbrowse2/gff/snps_hg19.csv -O snps_hg19.csv
#   awk '{print $4,$5,$9,$11,$12}' FS='","' snps_hg19.csv | sort -nk1 | \
#   awk 'NR>1 {if (a!=$1) {print a,b,c,d,e,f; c=""}} {x=$0;a=$1;b=$2;c=length(c)>0?c"/"$3:$3;d=$4;e=$5;f=$6}' OFS='\t' > snps_hg19.tsv
# to convert it to TSV format with one line per position. Unfortunately,
# there doesn't seem to be a simple Pythonic way of reading in the original CSV
# file due to the quotes.




backup_files: variant-*.txt snps_hg19.csv snp-names.csv snp-used.csv report.csv short-report.csv clades.csv tree.txt raw-ages.txt

haplogroup: R
top_snp: U106

# skip_to: this allows you to start the program at a particular point
#     0 -> run whole programme
#    10 -> work only with already-unzipped files
#    20 -> do not re-extract the VCF calls
#    30 -> do not re-make the tree
skip_to: 11
skip_to: 0


make_report: True
make_ages: True
make_short_report: True
make_html_report: True

